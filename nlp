1. Using spaCy

    With Anaconda add spaCy to your env.
    or
$ conda activate <env>
$ conda install -c conda-forge spacy
$ python -m spacy info

  From: https://spacy.io/usage/models
$ python -m spacy download en_core_web_sm
>>> import spacy
>>> nlp = spacy.load("en_core_web_sm")

(some problems downloading the small model and then using similarity. Download the large model.)

$ python -m spacy download en_core_web_lg
>>> nlp = spacy.load("en_core_web_lg")
>>> nlp("dog").similarity(nlp("animal"))

text = "Wait till you hear this!"
doc = nlp(text)
for token in doc:               # source: https://course.spacy.io
    # Get the token text, part-of-speech tag and dependency label
    token_text = token.text
    token_pos = token.pos_
    token_dep = token.dep_
    # This is for formatting only
    print(f"{token_text:<12}{token_pos:<10}{token_dep:<10}")
for ent in doc.ents:
    # Print the entity text and its label
    print(ent.text, ent.label)
some_span = doc[1:3]
print(some_span.text)

>>> from spacy.matcher import Matcher
matcher = Matcher(nlp.vocab)
pattern = [
    {"TEXT": "iPhone", "POS": "DET", "OP": "?",} ,      #these three requirements for the first token match
    {"TEXT": "X"}                                       #one more requirement for the next token match
]
matcher.add("name for the pattern", None, pattern)      #this becomes part of the vocab; arbitrary pattern(s) on one line
matches = matcher(doc)
doc = nlp("It's the processing in language-processing")
pattern = [
    { "TEXT": "language", "POS": "NOUN"},
    { "TEXT": "-"},
    { "TEXT": "processing", "POS": "VERB",},            #change the VERB to NOUN for it to find the match
    ]
matcher = Matcher(nlp.vocab)
matcher.add("find language-processing", None, pattern)
matches = matcher(doc)
for match_id, start, end in matcher(doc):
    # Print pattern string name and text of matched span
    print(doc.vocab.strings[match_id], doc[start:end].text)
 
if one of your patterns (keys) is "LOWER", make sure the value in the dictionary is lower. e.g. "LOWER":"iphone" , not "LOWER":"iPhone"
list of patterns here: https://spacy.io/usage/rule-based-matching#adding-patterns-attributes

COUNTRIES = ['Afghanistan', 'Ã…land Islands', 'Albania',         # a few countries to illustrate the Phrase Matcher
             'Czech Republic',
             'Italy',
             'Slovenia',
             'United Arab Emirates',
             'Saudi Arabia',
             'Zambia', 'Zimbabwe']
nlp = English()
doc = nlp("Saudi Arabia and United Arab Emirates are both in the Middle East")
from spacy.matcher import PhraseMatcher
matcher = PhraseMatcher(nlp.vocab)
patterns = list(nlp.pipe(COUNTRIES))
matcher.add("COUNTRY_ENTITY", None, *patterns)
matches = matcher(doc)
print( [ (nlp.vocab.strings[match_id], doc[start:end]) for match_id, start, end in matches])
print( doc.ents)
for match_id, start, end in matcher(doc):
    span = Span(doc, start, end, label="GPE")
    doc.ents = list(doc.ents) + [span]
    span_root_head = span.root.head
    print(span_root_head.text, "-->", span.text)
print([(ent.text, ent.label_) for ent in doc.ents if ent.label_ == "GPE"])





>>> from spacy.lang.en import English
>>> nlp = English()
or
>>> import spacy
>>> nlp = spacy.load("en_core_web_lg")
THEN you can obtain the internal hash used for cat in the two-way lookup StringStore table
cat_hash = nlp.vocab.strings["cat"]
cat_string = nlp.vocab.strings[cat_hash]

>>> nlp.vocab.strings["PERSON"]
380
>>> nlp.vocab.strings["person"]
14800503047316267216

# doc exposes vocab also, but basically points to the nlp vocab
>>> print ( len (doc.vocab.strings))
1476045
>>> print ( len (nlp.vocab.strings))
1476045
>>> print ( id (nlp.vocab.strings))
5571244880
>>> print ( id (doc.vocab.strings))
5571244880

from spacy.tokens import Doc, Span
#create a Doc from scratch
words = ["California", "is", "sunny", "!"]
spaces = [True, True, False, False]         # or spaces = None, to give space to all tokens
# Create a Doc from the words and spaces
doc = Doc(nlp.vocab, words=words, spaces=spaces)

doc = nlp("California is a state.")
span = Span(doc, 0, 1, label = "PNOUN")
# Add the span to the doc's entities
doc.ents = [span]
print([(ent.text, ent.label_) for ent in doc.ents])

>>> doc[0].vector
array([-3.0439e-01,  4.9227e-01, -7.6365e-02, -7.5354e-01,  5.8803e-01,
..
       -2.5008e-01,  3.6174e-01, -6.3967e-01,  6.6353e-02,  5.3749e-01],
      dtype=float32)
>>>

#Two different ways to get spans
span1 = Span(doc, 3, 5)
span2 = doc[-4:-1]
print (span1, "similarity to",  span2)
# Get the similarity of the spans
print (span1.similarity(span2))

Pipelines
=========

>>> nlp = English()
>>> nlp.pipe_names
[]
>>> nlp.pipeline
[]
>>> nlp= spacy.load("en_core_web_sm")
>>> nlp.pipe_names
['tagger', 'parser', 'ner']
>>> nlp.pipeline
[('tagger', <spacy.pipeline.pipes.Tagger object at 0x11dda5a10>), ('parser', <spacy.pipeline.pipes.DependencyParser object at 0x11a52afa0>), ('ner', <spacy.pipeline.pipes.EntityRecognizer object at 0x12a5d7210>)]

Customize Pipeline
------------------
def custom_component(doc):
    #e.g.
    #matches = matcher(doc)
    #spans = [Span(doc, start, end, label=match_id) for match_id, start, end in matches]
    #doc.ents = spans
    return doc
nlp.add_pipe(custom_component, ___) #the position of First/Last (boolean) or before/after (reference another component)


Extension Attributes
--------------------
#extend Token
from spacy.tokens import Token                      # Attribute Extension
nlp=English(); doc = nlp("Here is some text for you.")

# Attribute Extension
Token.set_extension("some_attr", default=False)     # adds attribute with default value; these hold values
doc[3]._.some_attr = True
print([(token.text, token._.some_attr) for token in doc])

#Property Extension
def get_reversed(token):                            # using getter, defers the calculation - a property extension
    return token.text[::-1]                         # can have optional setter
Token.set_extension("reversed", getter=get_reversed)
for token in doc:
    print("reversed:", token._.reversed)            # no parentheses

#extend Doc
from spacy.tokens import Doc                        # from spacy training
def get_has_number(doc):
    # Return if any of the tokens in the doc return True for token.like_num
    return any(token.like_num for token in doc)
Doc.set_extension("has_number", getter=get_has_number)      #notice setting this to class Doc, not Token
# Process the text and check the custom has_number attribute
doc = nlp("The museum closed for five years a decade ago.") 
print("has_number:", doc._.has_number)

#extend Span
from spacy.tokens import Span
def to_html(span, tag):
    # Wrap the span text in a HTML tag and return it
    return f"<{tag}>{span.text}</{tag}>"
Span.set_extension("to_html", method=to_html)
doc = nlp("Hello world, this is a sentence.")
span = doc[0:2]
print(span._.to_html("strong"))                     # calling a method ()

def get_wikipedia_url(span):
    # Get a Wikipedia URL if the span has one of the labels
    if span.label_ in ("PERSON", "ORG", "GPE", "LOCATION", "LOC"):          #added LOC
        entity_text = span.text.replace(" ", "_")
        return "https://en.wikipedia.org/w/index.php?search=" + entity_text
# Set the Span extension wikipedia_url using get getter get_wikipedia_url
Span.set_extension("wikipedia_url", getter=get_wikipedia_url)           #when rerunning this line, you may need to use force=True
doc = nlp(
    "John said he had frequently traveled to Europe, Africa, and had always used"
    "his time machine so that he would arrive not only in a specific location"
    "but in a specific time as well."
)
for ent in doc.ents:
    # Print the text and Wikipedia URL of the entity
    print(ent.text, ent._.wikipedia_url)


Performance
===========

for text in TEXTS:
    doc = nlp(text)
    #... more stuff per doc

for doc in nlp.pipe(TEXTS)      # faster; nlp.pipe creates a gen, so if you want a list wrap it in list()
    #... more stuff per doc

if you have DATA that looks like a list of lists, the latter like: [text, context_dict]
 e.g. [  ["Some text", { "param1 key": "param1 value", "param2 key": "param value", ...} ], ...]
for doc, context in nlp.pipe(DATA, as_tuples=True):


