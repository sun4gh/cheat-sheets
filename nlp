1. Using spaCy

    With Anaconda add spaCy to your env.
    or
$ conda activate <env>
$ conda install -c conda-forge spacy
$ python -m spacy info

  From: https://spacy.io/usage/models
$ python -m spacy download en_core_web_sm
>>> import spacy
>>> nlp = spacy.load("en_core_web_sm")

(some problems downloading the small model and then using similarity. Download the large model.)

$ python -m spacy download en_core_web_lg
>>> nlp = spacy.load("en_core_web_lg")
>>> nlp("dog").similarity(nlp("animal"))

text = "Wait till you hear this!"
doc = nlp(text)
for token in doc:               # source: https://course.spacy.io
    # Get the token text, part-of-speech tag and dependency label
    token_text = token.text
    token_pos = token.pos_
    token_dep = token.dep_
    # This is for formatting only
    print(f"{token_text:<12}{token_pos:<10}{token_dep:<10}")
for ent in doc.ents:
    # Print the entity text and its label
    print(ent.text, ent.label)
some_span = doc[1:3]
print(some_span.text)

>>> from spacy.matcher import Matcher
matcher = Matcher(nlp.vocab)
pattern = [
    {"TEXT": "iPhone", "POS": "DET", "OP": "?",} ,      #these three requirements for the first token match
    {"TEXT": "X"}                                       #one more requirement for the next token match
]
matcher.add("name for the pattern", None, pattern)      #this becomes part of the vocab
matches = matcher(doc)
doc = nlp("It's the processing in language-processing")
pattern = [
    { "TEXT": "language", "POS": "NOUN"},
    { "TEXT": "-"},
    { "TEXT": "processing", "POS": "VERB",},            #change the VERB to NOUN for it to find the match
    ]
matcher = Matcher(nlp.vocab)
matcher.add("find language-processing", None, pattern)
matches = matcher(doc)
for match_id, start, end in matcher(doc):
    # Print pattern string name and text of matched span
    print(doc.vocab.strings[match_id], doc[start:end].text)
 
COUNTRIES = ['Afghanistan', 'Ã…land Islands', 'Albania',         # a few countries to illustrate the Phrase Matcher
             'Czech Republic',
             'Italy',
             'Slovenia',
             'United Arab Emirates',
             'Saudi Arabia',
             'Zambia', 'Zimbabwe']
nlp = English()
doc = nlp("Saudi Arabia and United Arab Emirates are both in the Middle East")
from spacy.matcher import PhraseMatcher
matcher = PhraseMatcher(nlp.vocab)
patterns = list(nlp.pipe(COUNTRIES))
matcher.add("COUNTRY_ENTITY", None, *patterns)
matches = matcher(doc)
print( [ (nlp.vocab.strings[match_id], doc[start:end]) for match_id, start, end in matches])
print( doc.ents)
for match_id, start, end in matcher(doc):
    span = Span(doc, start, end, label="GPE")
    doc.ents = list(doc.ents) + [span]
    span_root_head = span.root.head
    print(span_root_head.text, "-->", span.text)
print([(ent.text, ent.label_) for ent in doc.ents if ent.label_ == "GPE"])





>>> from spacy.lang.en import English
>>> nlp = English()
or
>>> import spacy
>>> nlp = spacy.load("en_core_web_lg")
THEN you can obtain the internal hash used for cat in the two-way lookup StringStore table
cat_hash = nlp.vocab.strings["cat"]
cat_string = nlp.vocab.strings[cat_hash]

>>> nlp.vocab.strings["PERSON"]
380
>>> nlp.vocab.strings["person"]
14800503047316267216

# doc exposes vocab also, but basically points to the nlp vocab
>>> print ( len (doc.vocab.strings))
1476045
>>> print ( len (nlp.vocab.strings))
1476045
>>> print ( id (nlp.vocab.strings))
5571244880
>>> print ( id (doc.vocab.strings))
5571244880

from spacy.tokens import Doc, Span
#create a Doc from scratch
words = ["California", "is", "sunny", "!"]
spaces = [True, True, False, False]         # or spaces = None, to give space to all tokens
# Create a Doc from the words and spaces
doc = Doc(nlp.vocab, words=words, spaces=spaces)

doc = nlp("California is a state.")
span = Span(doc, 0, 1, label = "PNOUN")
# Add the span to the doc's entities
doc.ents = [span]
print([(ent.text, ent.label_) for ent in doc.ents])

>>> doc[0].vector
array([-3.0439e-01,  4.9227e-01, -7.6365e-02, -7.5354e-01,  5.8803e-01,
..
       -2.5008e-01,  3.6174e-01, -6.3967e-01,  6.6353e-02,  5.3749e-01],
      dtype=float32)
>>>

#Two different ways to get spans
span1 = Span(doc, 3, 5)
span2 = doc[-4:-1]
print (span1, "similarity to",  span2)
# Get the similarity of the spans
print (span1.similarity(span2))

Pipelines
=========

>>> nlp = English()
>>> nlp.pipe_names
[]
>>> nlp.pipeline
[]
>>> nlp= spacy.load("en_core_web_sm")
>>> nlp.pipe_names
['tagger', 'parser', 'ner']
>>> nlp.pipeline
[('tagger', <spacy.pipeline.pipes.Tagger object at 0x11dda5a10>), ('parser', <spacy.pipeline.pipes.DependencyParser object at 0x11a52afa0>), ('ner', <spacy.pipeline.pipes.EntityRecognizer object at 0x12a5d7210>)]

Customize Pipeline
------------------
def custom_component(doc):
    #e.g.
    #matches = matcher(doc)
    #spans = [Span(doc, start, end, label=match_id) for match_id, start, end in matches]
    #doc.ents = spans
    return doc
nlp.add_pipe(custom_component, ___) #the position of First/Last (boolean) or before/after (reference another component)
